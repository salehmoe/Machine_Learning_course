\documentclass{article}
\usepackage{helvet}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{color}
\usepackage[left=1in,top=1in,right=1in,bottom=1in,nohead,nofoot]{geometry}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\begin{document}
\fontfamily{phv}

\begin{flushright}
\textbf{Mohammad Saleh\\
GRA at PHY WSU\\
m.saleh@cern.ch\\ fa9520@wayne.edu\\
April 14, 2017}
\end{flushright}

\begin{center}
\textbf{CSC 5825 \\
Homework  4} \\
\end{center}

\section*{Solutions:}
\textbf{Question 1:}
\newline
\\
\textbf{Comparison}\\
In single-link, also known as the nearest neighbor technique or minimum method, in this
technique the distance between clusters is defined as the distance
between the closest pair, where only pairs consistung of one object
from each cluster is considered. In complete-link, also known as farthest
neighbor, it is opposite of single linkage, distance between clusters is
 defined as the distance between the most distanct pair of object, one from
  each group. In average-linkage, the distance between two clusters is defined
   as the average distance between all pairs of object, where pair is made up
    of one object from each cluster.\\
\textbf{Advantages, Disadvantages}\\
The disdvantege of single-linkage is that sometimes it can produce chaining
among the clusters where several clusters may be joined together simply because one of their
cases is within close proximity of case from a separate
cluster, this chaining effect can have disastrous
effects on the cluster solution, also it is more sensitive to noise and
outliners whcih can produce long, elongated clusters, where it advantages
over the other is that it can handle non-elliptical shapes clusters. The
complete-link advantages is that it is less susceptible to noise and produce
 more balanced clusters and it's disadvantages that it breaks large clusters
 and all of the clusters tends to have the same diameter, where small diameter
  clusters merged with large ones. For the average-linkage which compromise
  between Single and Complete-link, its main advantage that it is less susceptible
   to noise and outliers where its disadvantages is that it is biased towards
   globular clusters.\\
\\
\\
\\
\\
\textbf{Question 2}
\\
The solution for the two parts is in the figure \ref{Tree}.
\begin{figure}
  \includegraphics[scale=0.5]{Tree.pdf}
  \caption{Tree stump.}
\label{Tree}
\end{figure}
\\
\\
\textbf{Programming Questions}
\\
\\
\textbf{Question 1:}
\newline
Below is the syntax for my code and figure.\ref{MLP_snap} shows a snapshot of the compiled code the misclassification percentage error as we increase
the number of iterations this error decrease.
\begin{figure}
  \includegraphics[scale=0.3]{MLP_snap.png}
  \caption{snapshot of my code after compiling.}
\label{MLP_snap}
\end{figure}

\lstinputlisting[breaklines]{MLP.py}
\\
\textbf{Bonus question}\\
If we have direct weights from the input to the ouput and no activation function. It will be the same case for linear regression. So we end up looking at the
advantages of using linear regression over the MLP. So it will be helpful to compute the optimal model directly and efficiently becuase if we add an activation function, and possibly one hidden layers, you cannot compute an optimal model directly anymore,
and we are forced to use an iterative solution and no guarantee to converge and it will be slower. It can be trained as we usually do for linear regression and naive bayes in case of
discrete target. Where as the disadvantages of not using the hidden layers is that linear reggression can only model linear function, where if we add hidden layers, we can
approximate any continous function. 
\end{document}
